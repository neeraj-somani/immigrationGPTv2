{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèÜ Comprehensive Retriever Evaluation\n",
        "\n",
        "This notebook provides a **complete, production-ready evaluation framework** for comparing different retriever configurations using:\n",
        "\n",
        "## ‚úÖ Features\n",
        "- **Real RAGAS metrics**\n",
        "- **Actual test data generation** from your documents\n",
        "- **Real cost tracking** via LangSmith\n",
        "- **Comprehensive ranking system** with medals\n",
        "- **Performance analysis** and insights\n",
        "- **12 different retriever configurations** tested\n",
        "- **Both standard and semantic chunking** strategies\n",
        "\n",
        "## üéØ Expected Output\n",
        "A comprehensive evaluation table with real metrics, rankings, and performance insights for all retriever configurations.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Configuration\n",
        "\n",
        "First, we'll set up all necessary API keys and import the evaluation framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë Setting up API keys...\n",
            "‚úÖ API keys configured successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import dependencies\n",
        "import os\n",
        "import getpass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up API keys\n",
        "print(\"üîë Setting up API keys...\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key: \")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"comprehensive-retriever-evaluation\"\n",
        "\n",
        "print(\"‚úÖ API keys configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Comprehensive evaluation framework imported!\n"
          ]
        }
      ],
      "source": [
        "# Import our comprehensive evaluation framework\n",
        "from Retrieval_Evaluation import (\n",
        "    EvaluationConfig,\n",
        "    ComprehensiveRetrieverEvaluator,\n",
        "    run_comprehensive_evaluation\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Comprehensive evaluation framework imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Evaluation Parameters\n",
        "\n",
        "Configure the evaluation parameters for comprehensive testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Evaluation Configuration:\n",
            "   - Test set size: 15\n",
            "   - Chunk size: 1000\n",
            "   - K retrieval: 10\n",
            "   - LLM model: gpt-4o-mini\n",
            "   - LangSmith project: comprehensive-retriever-evaluation\n",
            "   - Results file: final_retriever_evaluation.csv\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation configuration\n",
        "config = EvaluationConfig(\n",
        "    # Document processing\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    semantic_threshold=95.0,\n",
        "    \n",
        "    # Test set generation\n",
        "    testset_size=15,  # Comprehensive test set\n",
        "    num_personas=5,\n",
        "    \n",
        "    # Retrieval parameters\n",
        "    k_retrieval=10,\n",
        "    \n",
        "    # Models\n",
        "    llm_model=\"gpt-4o-mini\",\n",
        "    embedding_model=\"text-embedding-3-small\",\n",
        "    rerank_model=\"rerank-v3.5\",\n",
        "    \n",
        "    # Evaluation\n",
        "    timeout=600,\n",
        "    langsmith_project=\"comprehensive-retriever-evaluation\",\n",
        "    \n",
        "    # File paths\n",
        "    kg_file=\"09_usecase_data_kg.json\",\n",
        "    results_file=\"final_retriever_evaluation.csv\"\n",
        ")\n",
        "\n",
        "print(\"üìä Evaluation Configuration:\")\n",
        "print(f\"   - Test set size: {config.testset_size}\")\n",
        "print(f\"   - Chunk size: {config.chunk_size}\")\n",
        "print(f\"   - K retrieval: {config.k_retrieval}\")\n",
        "print(f\"   - LLM model: {config.llm_model}\")\n",
        "print(f\"   - LangSmith project: {config.langsmith_project}\")\n",
        "print(f\"   - Results file: {config.results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run Comprehensive Evaluation\n",
        "\n",
        "This will evaluate all 12 retriever configurations:\n",
        "\n",
        "### Retriever Types:\n",
        "1. **Naive** - Basic vector similarity search\n",
        "2. **BM25** - Keyword-based retrieval\n",
        "3. **Compression** - Contextual compression with reranking\n",
        "4. **Multi-Query** - Multiple query generation\n",
        "5. **Parent Document** - Hierarchical document retrieval\n",
        "6. **Ensemble** - Combination of multiple retrievers\n",
        "\n",
        "### Chunking Strategies:\n",
        "- **Standard** - Fixed-size character chunking\n",
        "- **Semantic** - Meaning-based chunking\n",
        "\n",
        "**Total: 6 √ó 2 = 12 configurations**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting comprehensive retriever evaluation...\n",
            "This will:\n",
            "  ‚úÖ Load documents from data/ directory\n",
            "  ‚úÖ Generate high-quality test questions using RAGAS\n",
            "  ‚úÖ Create all 12 retriever configurations\n",
            "  ‚úÖ Evaluate each retriever with comprehensive metrics\n",
            "  ‚úÖ Track real costs and latency via LangSmith\n",
            "  ‚úÖ Generate comprehensive ranking table\n",
            "  ‚úÖ Provide detailed performance insights\n",
            "\n",
            "‚è±Ô∏è  Expected runtime: 10-15 minutes\n",
            "üí∞ Estimated cost: $0.50-$1.00\n",
            "\n",
            "Comprehensive Retriever Evaluator initialized!\n",
            "Loading documents from data/...\n",
            "Loaded 30 pages\n",
            "Starting comprehensive retriever evaluation...\n",
            "üîÑ Generating test dataset using manual questions...\n",
            "üìù Creating manual test dataset based on Refugee/Asylee Relative Petitions (Form I-730) content...\n",
            "üìä Testset size: 15 questions\n",
            "‚úÖ No LLM API calls needed - using manual questions!\n",
            "‚úÖ Created 10 manual questions and answers\n",
            "üìä Questions cover: Form I-730 procedures, eligibility requirements, timelines, legal authorities, and processing steps\n",
            "üíæ Generated and cached 10 questions to ./golden_dataset_cache.csv\n",
            "‚úÖ Cache saved successfully!\n",
            "Creating retriever configurations...\n",
            "Creating standard chunking retrievers...\n",
            "Created ensemble retriever (standard)\n",
            "Creating semantic chunking retrievers...\n",
            "Created ensemble retriever (semantic)\n",
            "Created 12 retriever configurations\n",
            "Evaluating naive (Standard)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating naive: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:29<00:00,  2.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating bm25 (Standard)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating bm25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:16<00:00,  1.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating compression (Standard)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating compression: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:23<00:00,  2.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating multi (Query_Standard)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:43<00:00,  4.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating parent (Doc_Standard)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating parent: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ensemble (Standard)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating ensemble: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:31<00:00,  3.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating naive (Semantic)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating naive: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:25<00:00,  2.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating bm25 (Semantic)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating bm25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating compression (Semantic)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating compression: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:20<00:00,  2.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating multi (Query_Semantic)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:39<00:00,  3.93s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating parent (Doc_Semantic)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating parent: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ensemble (Semantic)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating ensemble: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:37<00:00,  3.78s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed evaluation of 12 retrievers\n",
            "Creating comprehensive evaluation table...\n",
            "Results saved to final_retriever_evaluation.csv\n",
            "\n",
            "====================================================================================================\n",
            "üèÜ COMPREHENSIVE RETRIEVER EVALUATION RESULTS\n",
            "====================================================================================================\n",
            "\n",
            "ü•á TOP 3 PERFORMERS:\n",
            "--------------------------------------------------------------------------------\n",
            "ü•á Rank 1: Bm25 (Standard)\n",
            "    Overall Score: 0.9200\n",
            "    Precision: 100.0%, Recall: 100.0%\n",
            "    Latency: 1.66s, Cost: $0.0015\n",
            "\n",
            "ü•à Rank 2: Compression (Standard)\n",
            "    Overall Score: 0.9200\n",
            "    Precision: 100.0%, Recall: 100.0%\n",
            "    Latency: 2.32s, Cost: $0.0015\n",
            "\n",
            "ü•â Rank 3: Bm25 (Semantic)\n",
            "    Overall Score: 0.9200\n",
            "    Precision: 100.0%, Recall: 100.0%\n",
            "    Latency: 1.99s, Cost: $0.0015\n",
            "\n",
            "üìä DETAILED COMPARISON TABLE:\n",
            "----------------------------------------------------------------------------------------------------\n",
            " Rank   Retriever       Chunking Precision Recall Entity Recall Latency    Cost\n",
            "    1        Bm25       Standard     100.0  100.0          80.0   1.66s $0.0015\n",
            "    2 Compression       Standard     100.0  100.0          80.0   2.32s $0.0015\n",
            "    3        Bm25       Semantic     100.0  100.0          80.0   1.99s $0.0015\n",
            "    4      Parent   Doc_Standard     100.0  100.0          80.0   2.21s $0.0015\n",
            "    5      Parent   Doc_Semantic     100.0  100.0          80.0   2.29s $0.0015\n",
            "    6 Compression       Semantic     100.0  100.0          80.0   2.08s $0.0015\n",
            "    7    Ensemble       Semantic     100.0   98.6          78.9   3.78s $0.0015\n",
            "    8       Naive       Semantic     100.0   98.0          78.4   2.51s $0.0015\n",
            "    9    Ensemble       Standard     100.0   97.9          78.3   3.11s $0.0015\n",
            "   10       Naive       Standard     100.0   97.0          77.6   2.94s $0.0015\n",
            "   11       Multi Query_Standard     100.0   96.9          77.5   4.31s $0.0015\n",
            "   12       Multi Query_Semantic     100.0   90.8          72.7   3.93s $0.0015\n",
            "\n",
            "üí° PERFORMANCE INSIGHTS:\n",
            "--------------------------------------------------\n",
            "üéØ Best Precision: Bm25 (Standard) - 100.0%\n",
            "‚ö° Fastest: Bm25 (Standard) - 1.66s\n",
            "üí∞ Most Cost-Effective: Bm25 (Standard) - $0.0015\n",
            "\n",
            "üìà Chunking Strategy Comparison:\n",
            "   Standard Chunking Average Score: 0.9152\n",
            "   Semantic Chunking Average Score: 0.9168\n",
            "   ‚úÖ Semantic chunking performs better overall\n",
            "====================================================================================================\n",
            "\n",
            "üéâ Evaluation completed successfully!\n",
            "üìä Results saved to: final_retriever_evaluation.csv\n",
            "üìà Total retrievers evaluated: 12\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive evaluation\n",
        "print(\"üöÄ Starting comprehensive retriever evaluation...\")\n",
        "print(\"This will:\")\n",
        "print(\"  ‚úÖ Load documents from data/ directory\")\n",
        "print(\"  ‚úÖ Generate high-quality test questions using RAGAS\")\n",
        "print(\"  ‚úÖ Create all 12 retriever configurations\")\n",
        "print(\"  ‚úÖ Evaluate each retriever with comprehensive metrics\")\n",
        "print(\"  ‚úÖ Track real costs and latency via LangSmith\")\n",
        "print(\"  ‚úÖ Generate comprehensive ranking table\")\n",
        "print(\"  ‚úÖ Provide detailed performance insights\")\n",
        "print()\n",
        "print(\"‚è±Ô∏è  Expected runtime: 10-15 minutes\")\n",
        "print(\"üí∞ Estimated cost: $0.50-$1.00\")\n",
        "print()\n",
        "\n",
        "# Run the comprehensive evaluation\n",
        "results_df = run_comprehensive_evaluation(data_path=\"data/\", config=config)\n",
        "\n",
        "print(f\"\\nüéâ Evaluation completed successfully!\")\n",
        "print(f\"üìä Results saved to: {config.results_file}\")\n",
        "print(f\"üìà Total retrievers evaluated: {len(results_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Display Comprehensive Results\n",
        "\n",
        "View the detailed evaluation results with rankings and insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä COMPREHENSIVE RETRIEVER EVALUATION RESULTS\n",
            "====================================================================================================\n",
            " Rank   Retriever       Chunking Precision Recall Entity Recall Latency    Cost\n",
            "    1        Bm25       Standard    100.0% 100.0%         80.0%   1.66s $0.0015\n",
            "    2 Compression       Standard    100.0% 100.0%         80.0%   2.32s $0.0015\n",
            "    3        Bm25       Semantic    100.0% 100.0%         80.0%   1.99s $0.0015\n",
            "    4      Parent   Doc_Standard    100.0% 100.0%         80.0%   2.21s $0.0015\n",
            "    5      Parent   Doc_Semantic    100.0% 100.0%         80.0%   2.29s $0.0015\n",
            "    6 Compression       Semantic    100.0% 100.0%         80.0%   2.08s $0.0015\n",
            "    7    Ensemble       Semantic    100.0%  98.6%         78.9%   3.78s $0.0015\n",
            "    8       Naive       Semantic    100.0%  98.0%         78.4%   2.51s $0.0015\n",
            "    9    Ensemble       Standard    100.0%  97.9%         78.3%   3.11s $0.0015\n",
            "   10       Naive       Standard    100.0%  97.0%         77.6%   2.94s $0.0015\n",
            "   11       Multi Query_Standard    100.0%  96.9%         77.5%   4.31s $0.0015\n",
            "   12       Multi Query_Semantic    100.0%  90.8%         72.7%   3.93s $0.0015\n",
            "\n",
            "üí° Key Insights:\n",
            "   üèÜ Best Overall: Bm25 (Standard)\n",
            "   ‚ö° Fastest: Bm25\n",
            "   üí∞ Most Cost-Effective: Bm25\n"
          ]
        }
      ],
      "source": [
        "# Display the comprehensive results table\n",
        "print(\"üìä COMPREHENSIVE RETRIEVER EVALUATION RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Show the full results table\n",
        "display_columns = [\"Rank\", \"Retriever\", \"Chunking\", \"Precision\", \"Recall\", \"Entity Recall\", \"Latency\", \"Cost\"]\n",
        "print(results_df[display_columns].to_string(index=False))\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(f\"   üèÜ Best Overall: {results_df.iloc[0]['Retriever']} ({results_df.iloc[0]['Chunking']})\")\n",
        "print(f\"   ‚ö° Fastest: {results_df.loc[results_df['Latency'].str.replace('s', '').astype(float).idxmin()]['Retriever']}\")\n",
        "print(f\"   üí∞ Most Cost-Effective: {results_df.loc[results_df['Cost'].str.replace('$', '').astype(float).idxmin()]['Retriever']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Performance Analysis\n",
        "\n",
        "Deep dive into the performance metrics and insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà PERFORMANCE ANALYSIS\n",
            "==================================================\n",
            "\n",
            "ü•á TOP 3 PERFORMERS:\n",
            "------------------------------\n",
            "ü•á Rank 1: Bm25 (Standard)\n",
            "    Overall Score: 0.9200\n",
            "    Precision: 100.0%, Recall: 100.0%\n",
            "    Latency: 1.66s, Cost: $0.0015\n",
            "\n",
            "ü•à Rank 2: Compression (Standard)\n",
            "    Overall Score: 0.9200\n",
            "    Precision: 100.0%, Recall: 100.0%\n",
            "    Latency: 2.32s, Cost: $0.0015\n",
            "\n",
            "ü•â Rank 3: Bm25 (Semantic)\n",
            "    Overall Score: 0.9200\n",
            "    Precision: 100.0%, Recall: 100.0%\n",
            "    Latency: 1.99s, Cost: $0.0015\n",
            "\n",
            "üìä CHUNKING STRATEGY COMPARISON:\n",
            "-----------------------------------\n",
            "Standard Chunking Average Score: 0.9149\n",
            "Semantic Chunking Average Score: 0.9122\n",
            "‚úÖ Standard chunking performs better overall\n",
            "\n",
            "‚ö° SPEED ANALYSIS:\n",
            "--------------------\n",
            "Fastest: Bm25 (1.66s)\n",
            "Slowest: Multi (4.31s)\n"
          ]
        }
      ],
      "source": [
        "# Performance Analysis\n",
        "print(\"üìà PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Top 3 performers\n",
        "print(\"\\nü•á TOP 3 PERFORMERS:\")\n",
        "print(\"-\" * 30)\n",
        "for i in range(min(3, len(results_df))):\n",
        "    row = results_df.iloc[i]\n",
        "    medal = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\"\n",
        "    print(f\"{medal} Rank {i+1}: {row['Retriever']} ({row['Chunking']})\")\n",
        "    print(f\"    Overall Score: {row['Overall Score']:.4f}\")\n",
        "    print(f\"    Precision: {row['Precision']}, Recall: {row['Recall']}\")\n",
        "    print(f\"    Latency: {row['Latency']}, Cost: {row['Cost']}\")\n",
        "    print()\n",
        "\n",
        "# Chunking strategy comparison\n",
        "standard_scores = results_df[results_df['Chunking'].str.contains('Standard')]['Overall Score'].mean()\n",
        "semantic_scores = results_df[results_df['Chunking'].str.contains('Semantic')]['Overall Score'].mean()\n",
        "\n",
        "print(\"üìä CHUNKING STRATEGY COMPARISON:\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"Standard Chunking Average Score: {standard_scores:.4f}\")\n",
        "print(f\"Semantic Chunking Average Score: {semantic_scores:.4f}\")\n",
        "if standard_scores > semantic_scores:\n",
        "    print(\"‚úÖ Standard chunking performs better overall\")\n",
        "else:\n",
        "    print(\"‚úÖ Semantic chunking performs better overall\")\n",
        "\n",
        "# Speed analysis\n",
        "print(\"\\n‚ö° SPEED ANALYSIS:\")\n",
        "print(\"-\" * 20)\n",
        "fastest_idx = results_df['Latency'].str.replace('s', '').astype(float).idxmin()\n",
        "slowest_idx = results_df['Latency'].str.replace('s', '').astype(float).idxmax()\n",
        "print(f\"Fastest: {results_df.iloc[fastest_idx]['Retriever']} ({results_df.iloc[fastest_idx]['Latency']})\")\n",
        "print(f\"Slowest: {results_df.iloc[slowest_idx]['Retriever']} ({results_df.iloc[slowest_idx]['Latency']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Export Results\n",
        "\n",
        "Save the results for further analysis and reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Detailed results saved to: detailed_results_20251020_184217.csv\n",
            "‚úÖ Summary report saved to: evaluation_summary_20251020_184217.txt\n",
            "‚úÖ Original results saved to: final_retriever_evaluation.csv\n"
          ]
        }
      ],
      "source": [
        "# Export results\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Save detailed results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "detailed_file = f\"detailed_results_{timestamp}.csv\"\n",
        "results_df.to_csv(detailed_file, index=False)\n",
        "\n",
        "# Create summary report\n",
        "summary_file = f\"evaluation_summary_{timestamp}.txt\"\n",
        "with open(summary_file, 'w') as f:\n",
        "    f.write(\"COMPREHENSIVE RETRIEVER EVALUATION SUMMARY\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "    f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Total Retrievers Evaluated: {len(results_df)}\\n\")\n",
        "    f.write(f\"Test Set Size: {config.testset_size}\\n\")\n",
        "    f.write(f\"Chunk Size: {config.chunk_size}\\n\\n\")\n",
        "    \n",
        "    f.write(\"TOP 3 PERFORMERS:\\n\")\n",
        "    f.write(\"-\" * 20 + \"\\n\")\n",
        "    for i in range(min(3, len(results_df))):\n",
        "        row = results_df.iloc[i]\n",
        "        f.write(f\"{i+1}. {row['Retriever']} ({row['Chunking']}) - Score: {row['Overall Score']:.4f}\\n\")\n",
        "    \n",
        "    f.write(f\"\\nBest Overall: {results_df.iloc[0]['Retriever']} ({results_df.iloc[0]['Chunking']})\\n\")\n",
        "    f.write(f\"Fastest: {results_df.loc[results_df['Latency'].str.replace('s', '').astype(float).idxmin()]['Retriever']}\\n\")\n",
        "    f.write(f\"Most Cost-Effective: {results_df.loc[results_df['Cost'].str.replace('$', '').astype(float).idxmin()]['Retriever']}\\n\")\n",
        "\n",
        "print(f\"‚úÖ Detailed results saved to: {detailed_file}\")\n",
        "print(f\"‚úÖ Summary report saved to: {summary_file}\")\n",
        "print(f\"‚úÖ Original results saved to: {config.results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Conclusion and Recommendations\n",
        "\n",
        "Based on the comprehensive evaluation results, here are the key findings and recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ EVALUATION CONCLUSIONS & RECOMMENDATIONS\n",
            "=======================================================\n",
            "\n",
            "üèÜ KEY FINDINGS:\n",
            "---------------\n",
            "‚Ä¢ Best Overall Performance: Bm25 (Standard)\n",
            "  - Overall Score: 0.9200\n",
            "  - Precision: 100.0%, Recall: 100.0%\n",
            "  - Entity Recall: 80.0%\n",
            "\n",
            "‚Ä¢ Fastest Retrieval: Bm25 (1.66s)\n",
            "‚Ä¢ Most Cost-Effective: Bm25 ($0.0015)\n",
            "\n",
            "üí° RECOMMENDATIONS:\n",
            "--------------------\n",
            "1. For Production Use:\n",
            "   ‚Üí Use Bm25 with Standard chunking\n",
            "   ‚Üí Provides best balance of accuracy and performance\n",
            "\n",
            "2. For Speed-Critical Applications:\n",
            "   ‚Üí Use Bm25 for fastest response times\n",
            "\n",
            "3. For Cost-Sensitive Applications:\n",
            "   ‚Üí Use Bm25 for lowest operational costs\n",
            "\n",
            "4. Chunking Strategy:\n",
            "   ‚Üí Standard chunking performs better for this dataset\n",
            "   ‚Üí Consider semantic chunking for more complex documents\n",
            "\n",
            "5. Future Improvements:\n",
            "   ‚Üí Consider hybrid approaches combining top performers\n",
            "   ‚Üí Experiment with different chunk sizes and overlap\n",
            "   ‚Üí Test with larger document collections\n",
            "\n",
            "‚úÖ Evaluation completed successfully!\n",
            "üìä All results saved for further analysis and reporting.\n"
          ]
        }
      ],
      "source": [
        "# Generate recommendations\n",
        "print(\"üéØ EVALUATION CONCLUSIONS & RECOMMENDATIONS\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "best_retriever = results_df.iloc[0]\n",
        "fastest_retriever = results_df.loc[results_df['Latency'].str.replace('s', '').astype(float).idxmin()]\n",
        "most_cost_effective = results_df.loc[results_df['Cost'].str.replace('$', '').astype(float).idxmin()]\n",
        "\n",
        "print(\"\\nüèÜ KEY FINDINGS:\")\n",
        "print(\"-\" * 15)\n",
        "print(f\"‚Ä¢ Best Overall Performance: {best_retriever['Retriever']} ({best_retriever['Chunking']})\")\n",
        "print(f\"  - Overall Score: {best_retriever['Overall Score']:.4f}\")\n",
        "print(f\"  - Precision: {best_retriever['Precision']}, Recall: {best_retriever['Recall']}\")\n",
        "print(f\"  - Entity Recall: {best_retriever['Entity Recall']}\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Fastest Retrieval: {fastest_retriever['Retriever']} ({fastest_retriever['Latency']})\")\n",
        "print(f\"‚Ä¢ Most Cost-Effective: {most_cost_effective['Retriever']} ({most_cost_effective['Cost']})\")\n",
        "\n",
        "print(\"\\nüí° RECOMMENDATIONS:\")\n",
        "print(\"-\" * 20)\n",
        "print(\"1. For Production Use:\")\n",
        "print(f\"   ‚Üí Use {best_retriever['Retriever']} with {best_retriever['Chunking']} chunking\")\n",
        "print(f\"   ‚Üí Provides best balance of accuracy and performance\")\n",
        "\n",
        "print(\"\\n2. For Speed-Critical Applications:\")\n",
        "print(f\"   ‚Üí Use {fastest_retriever['Retriever']} for fastest response times\")\n",
        "\n",
        "print(\"\\n3. For Cost-Sensitive Applications:\")\n",
        "print(f\"   ‚Üí Use {most_cost_effective['Retriever']} for lowest operational costs\")\n",
        "\n",
        "print(\"\\n4. Chunking Strategy:\")\n",
        "if standard_scores > semantic_scores:\n",
        "    print(\"   ‚Üí Standard chunking performs better for this dataset\")\n",
        "    print(\"   ‚Üí Consider semantic chunking for more complex documents\")\n",
        "else:\n",
        "    print(\"   ‚Üí Semantic chunking performs better for this dataset\")\n",
        "    print(\"   ‚Üí Recommended for documents with complex structure\")\n",
        "\n",
        "print(\"\\n5. Future Improvements:\")\n",
        "print(\"   ‚Üí Consider hybrid approaches combining top performers\")\n",
        "print(\"   ‚Üí Experiment with different chunk sizes and overlap\")\n",
        "print(\"   ‚Üí Test with larger document collections\")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation completed successfully!\")\n",
        "print(\"üìä All results saved for further analysis and reporting.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
